{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"wFgHa5GtAv1S"},"outputs":[],"source":["from langchain.embeddings.openai import OpenAIEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.text_splitter import CharacterTextSplitter\n","from langchain import OpenAI, VectorDBQA\n","from langchain.document_loaders import DirectoryLoader\n","from langchain.prompts import PromptTemplate\n","from langchain.chains.question_answering import load_qa_chain\n","import config\n","import logging"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Load documents from the specified directory using a DirectoryLoader object\n","loader = DirectoryLoader(config.FILE_DIR, glob='*.pdf')\n","documents = loader.load()\n","\n","# split the text to chuncks of of size 1000\n","text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n","# Split the documents into chunks of size 1000 using a CharacterTextSplitter object\n","texts = text_splitter.split_documents(documents)\n","\n","# Create a vector store from the chunks using an OpenAIEmbeddings object and a Chroma object\n","embeddings = OpenAIEmbeddings(openai_api_key=config.OPENAI_API_KEY)\n","docsearch = Chroma.from_documents(texts, embeddings)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NxHhDK5ywBDk"},"outputs":[],"source":["# Define a function named 'answer' that takes a string prompt and an optional directory path\n","# for persisting data. The function returns a string that represents the answer to the prompt.\n","def answer(prompt: str, persist_directory: str = config.PERSIST_DIR) -> str:\n","    \n","    # Log a message indicating that the function has started\n","    LOGGER.info(f\"Start answering based on prompt: {prompt}.\")\n","    \n","    # Create a prompt template using a template from the config module and input variables\n","    # representing the context and question.\n","    prompt_template = PromptTemplate(template=config.prompt_template, input_variables=[\"context\", \"question\"])\n","    \n","    # Load a QA chain using an OpenAI object, a chain type, and a prompt template.\n","    doc_chain = load_qa_chain(\n","        llm=OpenAI(\n","            openai_api_key = config.OPENAI_API_KEY,\n","            model_name=\"text-davinci-003\",\n","            temperature=0,\n","            max_tokens=300,\n","        ),\n","        chain_type=\"stuff\",\n","        prompt=prompt_template,\n","    )\n","    \n","    # Log a message indicating the number of chunks to be considered when answering the user's query.\n","    LOGGER.info(f\"The top {config.k} chunks are considered to answer the user's query.\")\n","    \n","    # Create a VectorDBQA object using a vector store, a QA chain, and a number of chunks to consider.\n","    qa = VectorDBQA(vectorstore=docsearch, combine_documents_chain=doc_chain, k=config.k)\n","    \n","    # Call the VectorDBQA object to generate an answer to the prompt.\n","    result = qa({\"query\": prompt})\n","    answer = result[\"result\"]\n","    \n","    # Log a message indicating the answer that was generated\n","    LOGGER.info(f\"The returned answer is: {answer}\")\n","    \n","    # Log a message indicating that the function has finished and return the answer.\n","    LOGGER.info(f\"Answering module over.\")\n","    return answer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import chat\n","import streamlit as st\n","from streamlit_chat import message\n","\n","#Creating the chatbot interface\n","st.title(\"LLM-Powered Chatbot for Intelligent Conversations\")\n","st.subheader(\"AVA-Abonia Virtual Assistant\")\n","\n","# Storing the chat\n","if 'generated' not in st.session_state:\n","    st.session_state['generated'] = []\n","\n","if 'past' not in st.session_state:\n","    st.session_state['past'] = []\n","\n","# Define a function to clear the input text\n","def clear_input_text():\n","    global input_text\n","    input_text = \"\"\n","\n","# We will get the user's input by calling the get_text function\n","def get_text():\n","    global input_text\n","    input_text = st.text_input(\"Ask your Question\", key=\"input\", on_change=clear_input_text)\n","    return input_text\n","\n","def main():\n","    user_input = get_text()\n","\n","    if user_input:\n","        output = chat.answer(user_input)\n","        # store the output \n","        st.session_state.past.append(user_input)\n","        st.session_state.generated.append(output)\n","\n","    if st.session_state['generated']:\n","        for i in range(len(st.session_state['generated'])-1, -1, -1):\n","            message(st.session_state[\"generated\"][i], key=str(i))\n","            message(st.session_state['past'][i], is_user=True, key=str(i) + '_user')\n","\n","# Run the app\n","if __name__ == \"__main__\":\n","    main()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"colab":{"private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":0}
